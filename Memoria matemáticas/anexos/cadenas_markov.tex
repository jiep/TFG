\chapter{Cadenas de Markov} \label{ap:markov}

Una cadena de Markov es un proceso estocástico discreto donde la probabilidad del suceso siguiente sólo depende del suceso actual.\\

Tienen multitud de aplicaciones en física, meteorología y genética, entre otras. Una aplicación a los rankings se puede encontrar en la Sección~\ref{sec:markov}.\\

A continuación, introducimos el concepto de cadena de Markov, enunciamos algunas de sus propiedades y clasificamos los estados de los que puede constar una cadena de Markov.

\begin{defi}
Un proceso estocástico se define como una colección indexada de variables aleatorias $\{X_t\}$, donde el índice $t$ toma valores en un conjunto $T$ dado.  
\end{defi}

\begin{defi}
Las condiciones en las que puede estar un proceso estocástico se denominan estados. La variable aleatoria $X_t$ representa el estado del sistema en el tiempo $t$. El conjunto de todos los estados de un proceso estocástico se denota con la letra $S$.
\end{defi}

\begin{defi}
Si $T$ es un conjunto conjunto discreto y el conjunto de estados es finito, entonces un proceso estocástico se dice que es de tiempo discreto con espacio de estados finito. 
\end{defi}

\begin{defi}
Un vector fila $\mathbf{p} = (p_1,p_2\dots,p_n)$ es un vector de probabilidad si todas sus componentes son no negativas y su suma es $1$, es decir,

\[ \sum\limits_{i=1}^{n} p_i = 1 \]
\end{defi}

\begin{ejemplo}
El vector $\mathbf{p} = (1/2, 1/4, 1/5, 19/30)$ es un vector de probabilidad porque $\frac{1}{2} + \frac{1}{4} + \frac{1}{5} + \frac{19}{30} = 1$, y todas sus componentes son mayores o iguales que $1$. \\
Sin embargo, el vector $\mathbf{q} = (1, 1/2, -2, 1/2)$ no es un vector de probabilidad porque tiene una componente negativa, $-2$.
\end{ejemplo}

\begin{defi}
Una matriz cuadrada $\mathbf{A} = (a_{ij})$ de tamaño $n \times n$ se dice estocástica si cada una de sus filas es un vector de probabilidad, es decir, si cada cada entrada de $\mathbf{A}$ es no negativa y la suma de las entradas de cada fila es $1$.
\end{defi}

\begin{ejemplo}
La siguiente matriz es una matriz estocástica 

\[\mathbf{A} = \left(\begin{array}{ccc}
1/2 & 1/2 & 0\\
0   & 2/3 & 1/3\\
1   &  0  & 0
\end{array}\right) \]

porque todas sus filas son vectores de probabilidad.
\end{ejemplo}

\begin{defi}
Una matriz estocástica $\mathbf{A}$ se dice regular si todas las entradas de $\mathbf{A}^n$ son positivas, para algún $n \in \N$.
\end{defi}

\begin{ejemplo}
La matriz estocástica

\[ \mathbf{A} = \left(\begin{array}{cc}
0 & 1\\
1/2 & 1/2
\end{array}\right) \]

es regular porque 

\[ \mathbf{A}^2 = \left(\begin{array}{cc}
1/2 & 1/2\\
1/4 & 3/4
\end{array}\right) \]

tiene cada entrada positiva.\\
Sin embargo, la matriz estocástica

\[ \mathbf{B} = \left(\begin{array}{cc}
1 & 0\\
1/2 & 1/2
\end{array}\right) \]

no es regular porque

\[ \mathbf{B}^2 = \left(\begin{array}{cc}
1 & 0\\
3/4 & 1/4
\end{array}\right) \]

tiene una entrada que no es positiva.
\end{ejemplo}

\begin{defi}
Un punto fijo $\mathbf{p}^*$ de una matriz estocástica $\mathbf{A}$ se define como la solución de la ecuación

\[ \mathbf{p}^* \mathbf{A} = \mathbf{p}^* \]
\end{defi}

\begin{ejemplo}
Si consideramos las matriz estocástica regular

\[ \mathbf{A} = \left(\begin{array}{ccc}
0 & 1 & 0\\
0 & 0 & 1\\
1/2 & 1/2 & 0
\end{array}\right) \]

El punto fijo $\mathbf{p}^* = (x,y,z)$ tiene que cumplir que $x+y+z = 1$, y $x,y,z \in [0,1]$. Resolviendo el sistema $\mathbf{p}^* \mathbf{A} = \mathbf{p}^*$, resulta que el punto fijo $p^* = (1/5, 2/5, 2/5)$.
\end{ejemplo}

\begin{teo}
Sea $\mathbf{A}$ una matriz estocástica regular. Entonces

\begin{enumerate}
\item $\mathbf{A}$ tiene un único vector de probabilidad $\mathbf{p}$, y todas las componentes de $\mathbf{p}$ son positivas.
\item La secuencias de matrices de potencias de $\mathbf{A}$, $\mathbf{A},\mathbf{A}^2,\mathbf{A}^3,\dots$ aproxima la matriz $\mathbf{B}$ cuyas filas están compuestas por el punto fijo $\mathbf{p}^*$.
\item Si $\mathbf{p}$ es un vector de probabilidad, entonces la secuencia $\mathbf{p A}, \mathbf{p} \mathbf{A}^2,\mathbf{p} \mathbf{A}^3,\dots$ aproxima el punto fijo $\mathbf{p}^*$.
\end{enumerate}
\end{teo}

\begin{defi}
Se dice que un proceso estocástico $\{X_t\}$ tiene la propiedad de Markov si 

\[ P(X_{t+1} = j | X_0 = k_0, X_1 = k_1,\dots, X_{t-1} = k_{t-1}, X_t = i) = P(X_{t+1} = j | X_t = j)\]

para todo $t$, y toda sucesión $i, k_0, k1, \dots, k_{t-1}$. Es decir, la probabilidad de un suceso futuro, sólo depende del evento actual. 
\end{defi}

\begin{defi}
Un proceso estocástico $\{X_t\}$ es una cadena de Markov si presenta la propiedad de Markov.
\end{defi}

\begin{defi}
Las probabilidades condicionales $P(X_{t+1} = j | X_t = i)$ de una cadena de Markov se llaman probabilidades de transición.
\end{defi}

\begin{defi}
Si dada una cadena de Markov, y para cada $i, j$ se cumple que 

\[ P(X_{t+1} | X_t = i) = P(X_1 = j | X_0 = i) \quad \text{para todo $t=1,2,...$} \]

se dice que las probabilidades son estacionarias.
\end{defi}

Las probabilidades estacionarias se denotan de la siguiente manera:

\begin{eqnarray}
p_{ij} = P(X_{t+1} = j | X_t = i)\\
p_{ij}^{(n)} = P(X_{t+n} = j | X_t = i)
\end{eqnarray}

Las probabilidades estacionarias se pueden representan mediante una matriz estocástica de la siguiente manera:

\[ \mathbf{P}^{(n)} = \left(\begin{array}{c c c c}
p_{11}^{(n)} & p_{12}^{(n)} & \dots & p_{1M}^{(n)}\\
p_{21}^{(n)} & p_{22}^{(n)} & \dots & p_{2M}^{(n)}\\
\dots        & \dots        & \dots & \dots\\
p_{M1}^{(n)} & p_{M2}^{(n)} & \dots & p_{MM}^{(n)}  
\end{array}\right) \]

donde $M$ es el número de estados de la cadena de Markov. Esta matriz se denomina matriz de transición.\\

Otra posible representación es mediante un grafo ponderado, cuyo peso es la probabilidad de ir del estado $i$ al estado $j$. Este grafo se denomina diagrama de  de transición de estados.

\begin{ejemplo} \label{ej:markov}
Si tenemos la cadena de Markov con el conjunto de estados $S = \{A,B,C\}$, y la matriz de transición $\mathbf{P}$ definida como

\[ \mathbf{P} = \begin{array}{cc}
& \begin{array}{ccc}
A \ \ \ & B & \ \ \ C
\end{array} \\
\begin{array}{c}
A\\
B\\
C
\end{array} & \left(\begin{array}{ccc}
0.65 & 0.3  & 0.05\\
0.1  & 0.75 & 0.15\\
0.01 & 0.19 & 0.8
\end{array}\right)
\end{array} \] 

El diagrama de transiciones de estados se puede ver en la Figura \ref{fig:markov}.

\begin{figure}[htb]
\centering
\ejemplomarkov
\caption{Diagrama de transición de estados del Ejemplo \ref{ej:markov}}
\label{fig:markov}
\end{figure}
\end{ejemplo}

Las ecuaciones de Chapman-Kolmogorov proporcionan un método para calcular las probabilidades de transición de $n$ pasos $p_{ij}^{(n)}$:

\begin{equation}
p_{ij}^{(n)} = \sum\limits_{k=1}^{M-1} p_{ik}^{(m)}p_{kj}^{(n-m)} \quad \forall i,j = 1,\dots M
\end{equation}

donde $M$ es el número de estados de la cadena de Markov. \\

En los casos especiales $m=1$ y $m = n-1$ se obtienen las siguientes expresiones:

\begin{eqnarray}
p_{ij}^{(n)} = \sum\limits_{k=1}^{M-1} p_{ik}p_{kj}^{(n-1)}\\
p_{ij}^{(n)} = \sum\limits_{k=1}^{M-1} p_{ik}^{n-1}p_{kj}
\end{eqnarray}

Estas dos ecuaciones permiten calcular la matriz $\mathbf{P}^n$ de transiciones ya que 

\begin{align*}
\mathbf{P}^{(n)} & = \mathbf{P} \mathbf{P}^{(n-1)} = \mathbf{P}^{(n-1)} \mathbf{P}\\
				 & = \mathbf{P} \mathbf{P}^{n-1} = \mathbf{P}^{n-1} \mathbf{P} \\
				 & = \mathbf{P}^n 
\end{align*} 

Esto permite descomponer la matriz $\mathbf{P}^n$, como $\mathbf{P}^n = \mathbf{H} \mathbf{J}^n \mathbf{H}^{-1}$, donde $\mathbf{J}$ es la matriz de Jordan, diagonal por cajas formada por los autovalores de $\mathbf{P}$ y $\mathbf{H}$ es una matriz invertible cuyas columnas son los autovectores de $\mathbf{P}$ asociados a los autovalores.

\begin{ejemplo} \label{ej:markov_completo}

En una determinada región se observa que los días secos y húmedos se suceden de acuerdo a la siguiente frecuencia: si un día es seco, el día siguiente será seco con una probabilidad $\frac{1}{2}$, y si un día es húmedo, el día también lo será con probabilidad $\frac{3}{4}$.\\

Sabiendo que hoy está húmedo, calcular la probabilidad asociada al clima que hará dentro de dos días, y la probabilidad de que dentro de $n$ días el clima sea seco. \\

La variable que describe la cadena de Markov es $X_t:$ clima de la región en el día $t$, con $t \in \N$. El conjunto de estados es $S = \{S, H\}$, donde $S$ denota un día seco, $H$ denota un día húmedo.\\

La matriz de transiciones viene dada por:

\[ \mathbf{P} = \left( \begin{array}{cc}
0.5 & 0.5\\
0.25 & 0.75
\end{array} \right) \]

y el diagrama de transición de estados en la Figura \ref{fig:markov_completo}.

\begin{figure}[htb]
\centering
\ejemplomarkovcompleto
\caption{Diagrama de transición de estados del Ejemplo \ref{ej:markov_completo}}
\label{fig:markov_completo}
\end{figure} 

La distribución inicial es $\mathbf{q} = (0, 1)$.\\

Calculamos la $\mathbf{P}^2$:

\[ \mathbf{P}^2 = \left( \begin{array}{cc}
0.375 & 0.625\\
0.3125 & 0.6875
\end{array} \right) \]

Entonces

\[ p^{(2)} = \mathbf{q P}^2 = (0,1) \left( \begin{array}{cc}
0.375 & 0.625\\
0.3125 & 0.6875
\end{array} \right) = (0.3125, 0.6875) \]

La probabilidad de que el clima sea seco a los dos días es de $0.3125$, y de que sea el clima es de $0.6875$.\\

Para determinar la probabilidad de que dentro de $n$ días el clima sea húmedo. Se busca una descomposición del tipo $\mathbf{P}^n = \mathbf{H} \mathbf{J}^n \mathbf{H}^{-1}$. Lo primero es calcular los autovalores de $P$ que son $\lambda_1 = 1$ y $\lambda_2 = \frac{1}{4}$.\\

Los autovectores asociados a $\lambda_1$ y $\lambda_2$ son respectivamente son $(1,1)$ y $(-2,1)$.\\

Por tanto, las matrices $\mathbf{J}$, $\mathbf{J}^n$ y $\mathbf{H}$ son 

\begin{align*}
\mathbf{J} = \left( \begin{array}{cc}
1 & 0\\
0 & 1/4
\end{array} \right), 
\quad 
\mathbf{J}^n = \left( \begin{array}{cc}
1 & 0\\
0 & 1/4^n
\end{array} \right),
\quad
\mathbf{H} = \left( \begin{array}{cc}
1 & -2\\
1 & 1
\end{array} \right)
\end{align*}

Por tanto

\begin{align*}
\mathbf{P}^n & = \mathbf{H} \mathbf{J}^n \mathbf{H}^{-1} = \left( \begin{array}{cc}
1 & -2\\
1 & 1
\end{array} \right) 
\left( \begin{array}{cc}
1 & 0\\
0 & 1/4^n
\end{array} \right)
\left( \begin{array}{cc}
1/3 & 2/3\\
-1/3 & 1/3
\end{array}
\right)\\
\mathbf{P}^n & = \left(\begin{array}{cc}
\frac{1}{3} + \frac{2}{3 \cdot 4^n} & \frac{2}{3} - \frac{2}{3 \cdot 4^n} \\
\frac{1}{3} - \frac{1}{3 \cdot 4^n} & \frac{2}{3} + \frac{1}{3 \cdot 4^n}
\end{array}\right)
\end{align*}

La probabilidad pedida es $p_{HS}^{(n)} = \frac{1}{3} - \frac{1}{3 \cdot 4^n}$.

\end{ejemplo}

A continuación, damos una clasificación de los estados de una cadena de Markov. La deducción completa de la clasificación de los estados se puede ver en~\cite{hillier2006introducción}.

\begin{defi}
Se dice que el estado $j$ es accesible desde el estado $i$ si $p_{ij}^{(n)} > 0$ para algún $n \geq 0$.
\end{defi}

\begin{defi}
Si el estado $j$ es accesible desde el estado $i$ y el estado $i$ es accesible desde el estado $j$, entonces se dice que los estados $i$ y $j$ se comunican.
\end{defi}

\begin{defi}
Una cadena de Markov es irreducible si todos sus estados se comunican.
\end{defi}

\begin{defi}
Un estado es transitorio si existe un estado $j$ que es accesible desde el estado $i$, pero no viceversa, es decir, el estado $i$ no es accesible desde el estado $j$ con $i \neq j$.
\end{defi}

\begin{defi}
Un estado es recurrente si no es transitorio.
\end{defi}

\begin{defi}
Un estado es absorbente si $p_{ii} = 1$.
\end{defi}